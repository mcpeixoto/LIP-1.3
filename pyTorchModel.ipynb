{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import torchvision  \n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import timeitout_features\n",
    "from tqdm import tqdmout_features\n",
    "import numpy as npout_features\n",
    "from sklearn.metrics import confusion_matrixout_features\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_preds, y_true, epsilon=np.finfo(np.float32).eps):\n",
    "    return - (1-y_true)* torch.log(1-y_preds+epsilon) - y_true * torch.log(y_preds+epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data=\"train\", directory=join(os.getcwd(),'NormalizedData.h5')):\n",
    "    # Data types = train, validate, test\n",
    "    print(\"[+] Loading Data..\")\n",
    "    tic=timeit.default_timer()\n",
    "\n",
    "    df = pd.read_hdf(\n",
    "                    directory,\n",
    "                    key=data,\n",
    "                    )\n",
    "    \n",
    "    toc=timeit.default_timer()\n",
    "    print(\"[+] Data Loaded in\", int(toc-tic), \"seconds!\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(df, size=200, debug=False):\n",
    "    # Data types = train, validate, test\n",
    "\n",
    "    if debug:\n",
    "        print(\"[Info] Len(data) =\", len(df))\n",
    "        print(\"[Info] Len(data) / Batch Size =\", len(df)/size)\n",
    "\n",
    "    # Select the data size\n",
    "    # TODO: Estou a perder um pouco de dados no final\n",
    "    batch = iter([\n",
    "            # Big tuple\n",
    "            (\n",
    "                i+1, # Batch number\n",
    "                torch.tensor(df.iloc[x:x+size].drop(columns=['index','Name', 'Weights', 'Label']).values, dtype=torch.float32).to(device), # Features Removi MissingET_Eta porque tava cheio de NaA (???)\n",
    "                torch.tensor(df['Label'].iloc[x:x+size].values, dtype=torch.float32).to(device),\n",
    "\n",
    "                torch.tensor((\n",
    "                np.where(\n",
    "                    df['Label'].iloc[x:x+size] == 0,\n",
    "                    df['Weights'].iloc[x:x+size] / df['Weights'].iloc[x:x+size][df['Label'].iloc[x:x+size] == 0].sum(),\n",
    "                    df['Weights'].iloc[x:x+size] / df['Weights'].iloc[x:x+size][df['Label'].iloc[x:x+size] == 1].sum(),\n",
    "                )\n",
    "                * df['Label'].iloc[x:x+size].shape[0]\n",
    "                / 2\n",
    "                ), dtype=torch.float32).to(device),\n",
    "                df.iloc[x:x+size]['Name'] #Nome\n",
    "        ) \n",
    "        \n",
    "         for i,x in enumerate(list(filter(lambda x: (x%(size+1) == 0) , [x for x in range(len(df))])))]) # Para size = 100 -> 0, 101, 202, 303, .. , 909, 1010, 1111\n",
    "\n",
    "\n",
    "    del df\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "\n",
    "        features = 68\n",
    "        self.layer1 = nn.Linear(in_features=features, out_features=256)\n",
    "        self.layer2 = nn.Linear(in_features=256, out_features=256)\n",
    "        \"\"\" self.layer3 = nn.Linear(in_features=500, out_features=500)\n",
    "        self.layer4 = nn.Linear(in_features=500, out_features=500)\n",
    "        self.layer5 = nn.Linear(in_features=500, out_features=200) \"\"\"\n",
    "        self.out = nn.Linear(in_features=256, out_features=1)\n",
    "\n",
    "    def forward(self, t):\n",
    "\n",
    "\n",
    "        t = self.layer1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        \n",
    "        t = self.layer2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "\n",
    "        t = self.out(t).to(device)\n",
    "        # print(t.shape)\n",
    "\n",
    "        t = torch.sigmoid(t)\n",
    "\n",
    "        return t\n",
    "\n",
    "Network = Network().to(device)\n"
   ]
  },
  {
   "source": [
    "# Training Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[+] Loading Data..\n",
      "[+] Data Loaded in 7 seconds!\n",
      "[+] Loading Data..\n",
      "[+] Data Loaded in 2 seconds!\n"
     ]
    }
   ],
   "source": [
    "#batch_number, features, target, weights, name \n",
    "train_data = data_loader(data='train')\n",
    "val_data = data_loader(data='validate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_data, val_data, batch_size = 256, epochs = 100, name=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"), patience=10):\n",
    "    optimizer = optim.Adam(Network.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"[+] Starting training..\")\n",
    "    tic=timeit.default_timer()\n",
    "\n",
    "    # For early stopping\n",
    "    patience_count = 0\n",
    "    best_loss = 99999999999\n",
    "\n",
    "    # Validation data prep.\n",
    "    _, val_features, val_label, val_weights, _ = next(create_batch(val_data, size=val_data.shape[0]))\n",
    "    del val_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Stop loop - Early stop\n",
    "        if patience_count >= patience:\n",
    "            print(\"Early Stoping break!\", patience, \"epochs without improvement.\")\n",
    "            break \n",
    "\n",
    "        # Tensorboard\n",
    "        writer = SummaryWriter(log_dir=join(os.getcwd(), \"TensorboardLogs\", name))\n",
    "\n",
    "        # Define variables\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        # TIME!\n",
    "        tic_e = timeit.default_timer()\n",
    "        for batch in create_batch(train_data, size=batch_size):\n",
    "            # batch -> (batch_number, Features, Label, Weights, Nome)\n",
    "\n",
    "\n",
    "            preds = Network(batch[1]) # Passar a batch pela network\n",
    "\n",
    "            ## Loss\n",
    "            loss = bce(preds.squeeze(1), batch[2])\n",
    "            loss = (batch[3] * loss) / batch[3].sum()\n",
    "            loss = torch.mean(loss, dtype=torch.float32)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad() # Resetar gradientes\n",
    "            loss.backward() # Calcular os gradientes\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics for batch\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, batch[2])\n",
    "\n",
    "\n",
    "        # Validation Loss\n",
    "        with torch.no_grad():\n",
    "            val_loss = bce(Network(val_features).squeeze(1), val_label)\n",
    "            val_loss = (val_weights * loss) / val_weights.sum()\n",
    "            val_loss = torch.mean(val_loss, dtype=torch.float32)\n",
    "\n",
    "        ## Tensorboard, metrics and callbacks\n",
    "        # For every epoch, check if it's the best loss on the validation set to save the *best* model\n",
    "\n",
    "        if val_loss.item() < best_loss:\n",
    "            \n",
    "            # Save the best model\n",
    "            printout = \"Best model yet!\"\n",
    "\n",
    "            best_loss = val_loss.item()\n",
    "            \n",
    "            torch.save(Network.state_dict(), join(os.getcwd(),\"models\",name))\n",
    "\n",
    "            # Reset patience_count for early stop \n",
    "            patience_count = 0\n",
    "        else:\n",
    "            printout = \"\"\n",
    "            # Increase patience_count for early stop (one epoch without improvement)\n",
    "            patience_count += 1\n",
    "\n",
    "        \n",
    "        # Save metrics on the tensorboard logs\n",
    "        writer.add_scalar('Total Correct', total_correct/train_data.shape[0], epoch)\n",
    "        writer.add_scalar('Loss', total_loss, epoch)\n",
    "        writer.add_scalar('Val Loss',val_loss, epoch)\n",
    "\n",
    "        # TIME!\n",
    "        toc_e = timeit.default_timer()\n",
    "        print(\"Epoch\", epoch, \"| Total Correct:\", total_correct/train_data.shape[0] , \"| Loss:\", total_loss,  \"| Val Loss:\", val_loss.item(), \"| Time\", int((toc_e-tic_e)), \"seconds |\", printout )\n",
    "\n",
    "    # Final do treino\n",
    "    toc=timeit.default_timer()\n",
    "\n",
    "    print(\"[+] Training completed in\", int((toc-tic)/60), \"minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[+] Starting training..\n",
      "Epoch 0 | Total Correct: 0.9522519829896879 | Loss: 5.419519945324282 | Val Loss: 2.330949433471119e-09 | Time 21 seconds | Best model yet!\n",
      "Epoch 1 | Total Correct: 0.9522519829896879 | Loss: 5.067026061180513 | Val Loss: 1.5877614778503357e-09 | Time 20 seconds | Best model yet!\n",
      "Epoch 2 | Total Correct: 0.9522519829896879 | Loss: 4.934206225676462 | Val Loss: 1.3728733705420382e-09 | Time 23 seconds | Best model yet!\n",
      "Epoch 3 | Total Correct: 0.9522519829896879 | Loss: 4.795023772850982 | Val Loss: 1.1064863469911757e-09 | Time 21 seconds | Best model yet!\n",
      "Epoch 4 | Total Correct: 0.9522519829896879 | Loss: 4.679422979708761 | Val Loss: 9.16902997971647e-10 | Time 20 seconds | Best model yet!\n",
      "Epoch 5 | Total Correct: 0.9522519829896879 | Loss: 4.579642560755019 | Val Loss: 9.37795840982858e-10 | Time 20 seconds | \n",
      "Epoch 6 | Total Correct: 0.9522519829896879 | Loss: 4.4725878591270884 | Val Loss: 9.08740915850359e-10 | Time 23 seconds | Best model yet!\n",
      "Epoch 7 | Total Correct: 0.9522519829896879 | Loss: 4.376502936793258 | Val Loss: 8.531912398801467e-10 | Time 23 seconds | Best model yet!\n",
      "Epoch 8 | Total Correct: 0.9522519829896879 | Loss: 4.268179301870987 | Val Loss: 7.57692575348301e-10 | Time 22 seconds | Best model yet!\n",
      "Epoch 9 | Total Correct: 0.9522519829896879 | Loss: 4.178021912448457 | Val Loss: 6.888193349041671e-10 | Time 22 seconds | Best model yet!\n",
      "Epoch 10 | Total Correct: 0.9522519829896879 | Loss: 4.105964807851706 | Val Loss: 7.959378711674958e-10 | Time 24 seconds | \n",
      "Epoch 11 | Total Correct: 0.9522519829896879 | Loss: 4.008628714916995 | Val Loss: 7.940320623234243e-10 | Time 24 seconds | \n",
      "Epoch 12 | Total Correct: 0.9522519829896879 | Loss: 3.8911647498462116 | Val Loss: 7.188258321910723e-10 | Time 23 seconds | \n",
      "Epoch 13 | Total Correct: 0.9522519829896879 | Loss: 3.820750248618424 | Val Loss: 7.039680505194212e-10 | Time 23 seconds | \n",
      "Epoch 14 | Total Correct: 0.9522519829896879 | Loss: 3.7267282934626564 | Val Loss: 7.435245752418496e-10 | Time 23 seconds | \n",
      "Epoch 15 | Total Correct: 0.9522519829896879 | Loss: 3.6277249886625214 | Val Loss: 7.978223637294946e-10 | Time 23 seconds | \n",
      "Epoch 16 | Total Correct: 0.9522519829896879 | Loss: 3.547803800945985 | Val Loss: 5.422381432751422e-10 | Time 23 seconds | Best model yet!\n",
      "Epoch 17 | Total Correct: 0.9522519829896879 | Loss: 3.489344926791091 | Val Loss: 4.3755696288449997e-10 | Time 23 seconds | Best model yet!\n",
      "Epoch 18 | Total Correct: 0.9522519829896879 | Loss: 3.420027642896457 | Val Loss: 6.458013568355625e-10 | Time 23 seconds | \n",
      "Epoch 19 | Total Correct: 0.9522519829896879 | Loss: 3.3339191825361922 | Val Loss: 8.239847693047864e-10 | Time 23 seconds | \n",
      "Epoch 20 | Total Correct: 0.9522519829896879 | Loss: 3.2721005260173115 | Val Loss: 5.225956889454153e-10 | Time 23 seconds | \n",
      "Epoch 21 | Total Correct: 0.9522519829896879 | Loss: 3.1991192951609264 | Val Loss: 8.072909563061614e-10 | Time 21 seconds | \n",
      "Epoch 22 | Total Correct: 0.9522519829896879 | Loss: 3.1315298409826937 | Val Loss: 5.451623596997024e-10 | Time 20 seconds | \n",
      "Epoch 23 | Total Correct: 0.9522519829896879 | Loss: 3.0789541184785776 | Val Loss: 6.125802087808552e-10 | Time 20 seconds | \n",
      "Epoch 24 | Total Correct: 0.9522519829896879 | Loss: 3.014350833298522 | Val Loss: 5.863045604570516e-10 | Time 20 seconds | \n",
      "Epoch 25 | Total Correct: 0.9522519829896879 | Loss: 2.9660822442019708 | Val Loss: 3.639900880703095e-10 | Time 20 seconds | Best model yet!\n",
      "Epoch 26 | Total Correct: 0.9522519829896879 | Loss: 2.920975453700521 | Val Loss: 4.0141656665326764e-10 | Time 21 seconds | \n",
      "Epoch 27 | Total Correct: 0.9522519829896879 | Loss: 2.865431939069822 | Val Loss: 3.528217162873659e-10 | Time 23 seconds | Best model yet!\n",
      "Epoch 28 | Total Correct: 0.9522519829896879 | Loss: 2.83814659849304 | Val Loss: 2.558782352668487e-10 | Time 23 seconds | Best model yet!\n",
      "Epoch 29 | Total Correct: 0.9522519829896879 | Loss: 2.79115399443981 | Val Loss: 2.103837937195152e-10 | Time 22 seconds | Best model yet!\n",
      "Epoch 30 | Total Correct: 0.9522519829896879 | Loss: 2.7565631197430776 | Val Loss: 3.039251350589467e-10 | Time 21 seconds | \n",
      "Epoch 31 | Total Correct: 0.9522519829896879 | Loss: 2.7222219338509603 | Val Loss: 3.7378211636962533e-10 | Time 24 seconds | \n",
      "Epoch 32 | Total Correct: 0.9522519829896879 | Loss: 2.7037588187813526 | Val Loss: 3.6790853696899717e-10 | Time 21 seconds | \n",
      "Epoch 33 | Total Correct: 0.9522519829896879 | Loss: 2.6705975929944543 | Val Loss: 2.532878351502177e-10 | Time 22 seconds | \n",
      "Epoch 34 | Total Correct: 0.9522519829896879 | Loss: 2.5902506444435858 | Val Loss: 2.1429233387770807e-10 | Time 21 seconds | \n",
      "Epoch 35 | Total Correct: 0.9522519829896879 | Loss: 2.570554579979216 | Val Loss: 2.776879559629464e-10 | Time 21 seconds | \n",
      "Epoch 36 | Total Correct: 0.9522519829896879 | Loss: 2.571309020939225 | Val Loss: 3.534685044659369e-10 | Time 21 seconds | \n",
      "Epoch 37 | Total Correct: 0.9522519829896879 | Loss: 2.5074920286861015 | Val Loss: 2.4241650353751254e-10 | Time 21 seconds | \n",
      "Epoch 38 | Total Correct: 0.9522519829896879 | Loss: 2.5134471004384977 | Val Loss: 3.0795924144122466e-10 | Time 21 seconds | \n",
      "Epoch 39 | Total Correct: 0.9522519829896879 | Loss: 2.469381785191217 | Val Loss: 3.113763691331428e-10 | Time 21 seconds | \n",
      "Early Stoping break! 10 epochs without improvement.\n",
      "[+] Training completed in 14 minutes!\n"
     ]
    }
   ],
   "source": [
    "name=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "train(train_data, val_data, name=name, epochs=500)\n",
    "name"
   ]
  },
  {
   "source": [
    "# Testing the model out"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f1e1000f5e0>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "device = device = torch.device('cpu')\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "Network = Network.to(device)\n",
    "Network.load_state_dict(torch.load(join(os.getcwd(),\"models\",name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (layer1): Linear(in_features=68, out_features=256, bias=True)\n",
       "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "\n",
    "Network.eval()"
   ]
  },
  {
   "source": [
    "### Load Test Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[+] Loading Data..\n",
      "[+] Data Loaded in 2 seconds!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Jet_Multi     Jet1_Mass      Jet1_Eta      Jet1_Phi       Jet1_PT  \\\n",
       "count  1.509872e+06  1.509872e+06  1.509872e+06  1.509872e+06  1.509872e+06   \n",
       "mean  -4.403962e-04 -3.169950e-04  1.919765e-03 -7.500912e-04 -2.182479e-03   \n",
       "std    9.997421e-01  9.999649e-01  1.001273e+00  9.997466e-01  9.951093e-01   \n",
       "min   -1.649581e+00 -1.384238e+00 -3.538465e+00 -1.732104e+00 -1.227972e+00   \n",
       "25%   -1.024832e+00 -5.923256e-01 -6.987161e-01 -8.670166e-01 -6.325989e-01   \n",
       "50%   -4.000820e-01 -2.591055e-01  1.524748e-03 -1.526841e-03 -2.186928e-01   \n",
       "75%    8.494175e-01  2.696276e-01  7.022158e-01  8.662250e-01  3.497483e-01   \n",
       "max    7.096915e+00  4.129075e+01  3.513001e+00  1.731554e+00  3.008994e+01   \n",
       "\n",
       "          Jet1_BTag     Jet2_Mass      Jet2_Eta      Jet2_Phi       Jet2_PT  \\\n",
       "count  1.509872e+06  1.509872e+06  1.509872e+06  1.509872e+06  1.509872e+06   \n",
       "mean  -2.956233e-04 -1.920912e-03 -1.473976e-04  5.886514e-05 -1.119410e-03   \n",
       "std    1.000060e+00  9.988197e-01  9.996049e-01  1.000510e+00  9.982385e-01   \n",
       "min   -1.222218e+00 -1.458502e+00 -3.168550e+00 -1.795324e+00 -1.484852e+00   \n",
       "25%   -1.222218e+00 -5.621344e-01 -6.583196e-01 -8.318531e-01 -6.321707e-01   \n",
       "50%    8.181844e-01 -1.839713e-01 -5.016521e-04  1.541433e-04 -1.812021e-01   \n",
       "75%    8.181844e-01  3.662726e-01  6.576045e-01  8.318921e-01  4.305953e-01   \n",
       "max    8.181844e-01  3.596933e+01  3.166699e+00  1.795633e+00  3.140511e+01   \n",
       "\n",
       "       ...     Muon1_Phi      Muon1_PT     Muon2_Eta     Muon2_Phi  \\\n",
       "count  ...  1.509872e+06  1.509872e+06  1.509872e+06  1.509872e+06   \n",
       "mean   ...  1.310890e-03 -3.390498e-03  1.056866e-03  2.244645e-04   \n",
       "std    ...  9.998913e-01  9.984771e-01  9.992862e-01  9.994316e-01   \n",
       "min    ... -1.999019e+00 -9.487765e-01 -3.067431e+00 -2.619935e+00   \n",
       "25%    ... -6.643602e-01 -9.487765e-01  6.259453e-04  2.053101e-04   \n",
       "50%    ...  7.188655e-04 -1.115985e-01  6.259453e-04  2.053101e-04   \n",
       "75%    ...  6.678170e-01  3.632169e-01  6.259453e-04  2.053101e-04   \n",
       "max    ...  2.000454e+00  3.957771e+01  3.068657e+00  2.620328e+00   \n",
       "\n",
       "           Muon2_PT  MissingET_MET  MissingET_Phi   ScalarHT_HT         Label  \\\n",
       "count  1.509872e+06   1.509872e+06   1.509872e+06  1.509872e+06  1.509872e+06   \n",
       "mean  -2.551370e-03  -9.862976e-04  -5.075757e-04 -1.545192e-03  4.409513e-02   \n",
       "std    9.930162e-01   9.968513e-01   9.988965e-01  9.976698e-01  2.053065e-01   \n",
       "min   -6.540112e-01  -1.136653e+00  -1.731221e+00 -1.597308e+00  0.000000e+00   \n",
       "25%   -6.540112e-01  -7.578464e-01  -8.644043e-01 -6.779826e-01  0.000000e+00   \n",
       "50%   -6.540112e-01  -2.311350e-01  -1.250374e-03 -2.020433e-01  0.000000e+00   \n",
       "75%    5.261358e-01   4.865845e-01   8.641697e-01  4.270109e-01  0.000000e+00   \n",
       "max    3.332523e+01   3.837758e+01   1.731338e+00  2.017072e+01  1.000000e+00   \n",
       "\n",
       "            Weights  \n",
       "count  1.509872e+06  \n",
       "mean   7.772088e-06  \n",
       "std    1.400269e-05  \n",
       "min    1.286454e-09  \n",
       "25%    1.811693e-07  \n",
       "50%    5.028781e-06  \n",
       "75%    7.089469e-06  \n",
       "max    8.083906e-05  \n",
       "\n",
       "[8 rows x 70 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Jet_Multi</th>\n      <th>Jet1_Mass</th>\n      <th>Jet1_Eta</th>\n      <th>Jet1_Phi</th>\n      <th>Jet1_PT</th>\n      <th>Jet1_BTag</th>\n      <th>Jet2_Mass</th>\n      <th>Jet2_Eta</th>\n      <th>Jet2_Phi</th>\n      <th>Jet2_PT</th>\n      <th>...</th>\n      <th>Muon1_Phi</th>\n      <th>Muon1_PT</th>\n      <th>Muon2_Eta</th>\n      <th>Muon2_Phi</th>\n      <th>Muon2_PT</th>\n      <th>MissingET_MET</th>\n      <th>MissingET_Phi</th>\n      <th>ScalarHT_HT</th>\n      <th>Label</th>\n      <th>Weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>...</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n      <td>1.509872e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-4.403962e-04</td>\n      <td>-3.169950e-04</td>\n      <td>1.919765e-03</td>\n      <td>-7.500912e-04</td>\n      <td>-2.182479e-03</td>\n      <td>-2.956233e-04</td>\n      <td>-1.920912e-03</td>\n      <td>-1.473976e-04</td>\n      <td>5.886514e-05</td>\n      <td>-1.119410e-03</td>\n      <td>...</td>\n      <td>1.310890e-03</td>\n      <td>-3.390498e-03</td>\n      <td>1.056866e-03</td>\n      <td>2.244645e-04</td>\n      <td>-2.551370e-03</td>\n      <td>-9.862976e-04</td>\n      <td>-5.075757e-04</td>\n      <td>-1.545192e-03</td>\n      <td>4.409513e-02</td>\n      <td>7.772088e-06</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9.997421e-01</td>\n      <td>9.999649e-01</td>\n      <td>1.001273e+00</td>\n      <td>9.997466e-01</td>\n      <td>9.951093e-01</td>\n      <td>1.000060e+00</td>\n      <td>9.988197e-01</td>\n      <td>9.996049e-01</td>\n      <td>1.000510e+00</td>\n      <td>9.982385e-01</td>\n      <td>...</td>\n      <td>9.998913e-01</td>\n      <td>9.984771e-01</td>\n      <td>9.992862e-01</td>\n      <td>9.994316e-01</td>\n      <td>9.930162e-01</td>\n      <td>9.968513e-01</td>\n      <td>9.988965e-01</td>\n      <td>9.976698e-01</td>\n      <td>2.053065e-01</td>\n      <td>1.400269e-05</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.649581e+00</td>\n      <td>-1.384238e+00</td>\n      <td>-3.538465e+00</td>\n      <td>-1.732104e+00</td>\n      <td>-1.227972e+00</td>\n      <td>-1.222218e+00</td>\n      <td>-1.458502e+00</td>\n      <td>-3.168550e+00</td>\n      <td>-1.795324e+00</td>\n      <td>-1.484852e+00</td>\n      <td>...</td>\n      <td>-1.999019e+00</td>\n      <td>-9.487765e-01</td>\n      <td>-3.067431e+00</td>\n      <td>-2.619935e+00</td>\n      <td>-6.540112e-01</td>\n      <td>-1.136653e+00</td>\n      <td>-1.731221e+00</td>\n      <td>-1.597308e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.286454e-09</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.024832e+00</td>\n      <td>-5.923256e-01</td>\n      <td>-6.987161e-01</td>\n      <td>-8.670166e-01</td>\n      <td>-6.325989e-01</td>\n      <td>-1.222218e+00</td>\n      <td>-5.621344e-01</td>\n      <td>-6.583196e-01</td>\n      <td>-8.318531e-01</td>\n      <td>-6.321707e-01</td>\n      <td>...</td>\n      <td>-6.643602e-01</td>\n      <td>-9.487765e-01</td>\n      <td>6.259453e-04</td>\n      <td>2.053101e-04</td>\n      <td>-6.540112e-01</td>\n      <td>-7.578464e-01</td>\n      <td>-8.644043e-01</td>\n      <td>-6.779826e-01</td>\n      <td>0.000000e+00</td>\n      <td>1.811693e-07</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-4.000820e-01</td>\n      <td>-2.591055e-01</td>\n      <td>1.524748e-03</td>\n      <td>-1.526841e-03</td>\n      <td>-2.186928e-01</td>\n      <td>8.181844e-01</td>\n      <td>-1.839713e-01</td>\n      <td>-5.016521e-04</td>\n      <td>1.541433e-04</td>\n      <td>-1.812021e-01</td>\n      <td>...</td>\n      <td>7.188655e-04</td>\n      <td>-1.115985e-01</td>\n      <td>6.259453e-04</td>\n      <td>2.053101e-04</td>\n      <td>-6.540112e-01</td>\n      <td>-2.311350e-01</td>\n      <td>-1.250374e-03</td>\n      <td>-2.020433e-01</td>\n      <td>0.000000e+00</td>\n      <td>5.028781e-06</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.494175e-01</td>\n      <td>2.696276e-01</td>\n      <td>7.022158e-01</td>\n      <td>8.662250e-01</td>\n      <td>3.497483e-01</td>\n      <td>8.181844e-01</td>\n      <td>3.662726e-01</td>\n      <td>6.576045e-01</td>\n      <td>8.318921e-01</td>\n      <td>4.305953e-01</td>\n      <td>...</td>\n      <td>6.678170e-01</td>\n      <td>3.632169e-01</td>\n      <td>6.259453e-04</td>\n      <td>2.053101e-04</td>\n      <td>5.261358e-01</td>\n      <td>4.865845e-01</td>\n      <td>8.641697e-01</td>\n      <td>4.270109e-01</td>\n      <td>0.000000e+00</td>\n      <td>7.089469e-06</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.096915e+00</td>\n      <td>4.129075e+01</td>\n      <td>3.513001e+00</td>\n      <td>1.731554e+00</td>\n      <td>3.008994e+01</td>\n      <td>8.181844e-01</td>\n      <td>3.596933e+01</td>\n      <td>3.166699e+00</td>\n      <td>1.795633e+00</td>\n      <td>3.140511e+01</td>\n      <td>...</td>\n      <td>2.000454e+00</td>\n      <td>3.957771e+01</td>\n      <td>3.068657e+00</td>\n      <td>2.620328e+00</td>\n      <td>3.332523e+01</td>\n      <td>3.837758e+01</td>\n      <td>1.731338e+00</td>\n      <td>2.017072e+01</td>\n      <td>1.000000e+00</td>\n      <td>8.083906e-05</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 70 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "test_data = data_loader(data=\"test\").drop(columns=['index'])\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = torch.tensor(test_data['Label'].values, dtype=torch.float32).to(device)\n",
    "test_predictions = Network(torch.tensor(test_data.drop(columns=['Label', 'Weights', 'Name']).values, dtype=torch.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1314039,    9120],\n",
       "       [ 129255,   57458]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "confusion_matrix(test_predictions.round(), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-02-14T11:35:46.548316</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m67755efa69\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m67755efa69\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(37.369744 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.194034\" xlink:href=\"#m67755efa69\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <g transform=\"translate(98.242472 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.066761\" xlink:href=\"#m67755efa69\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <g transform=\"translate(159.115199 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.939489\" xlink:href=\"#m67755efa69\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <g transform=\"translate(219.987926 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.812216\" xlink:href=\"#m67755efa69\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <g transform=\"translate(280.860653 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.684943\" xlink:href=\"#m67755efa69\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <g transform=\"translate(341.733381 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m548a263870\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m548a263870\" y=\"214.756364\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m548a263870\" y=\"175.221818\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 179.021037)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m548a263870\" y=\"135.687273\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 139.486491)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m548a263870\" y=\"96.152727\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 99.951946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m548a263870\" y=\"56.618182\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 60.417401)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m548a263870\" y=\"17.083636\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p14001b091d)\" d=\"M 45.321307 214.756364 \nL 45.534462 139.532773 \nL 45.634558 130.747384 \nL 45.752079 122.478609 \nL 45.779922 120.851575 \nL 45.89047 114.857084 \nL 45.907097 113.951527 \nL 46.016988 108.942757 \nL 46.024259 108.642884 \nL 46.142688 104.441692 \nL 46.144906 104.334807 \nL 46.144906 104.331838 \nL 46.255983 101.20247 \nL 46.271995 100.881814 \nL 46.383061 98.310625 \nL 46.652862 92.942602 \nL 46.658015 92.832747 \nL 46.768386 91.075076 \nL 46.780275 90.974128 \nL 46.890887 89.498515 \nL 46.891385 89.498515 \nL 46.917737 89.157076 \nL 47.027985 87.78538 \nL 47.028003 87.78538 \nL 47.040786 87.675525 \nL 47.150078 86.386962 \nL 47.151509 86.386962 \nL 47.159652 86.277107 \nL 47.269966 85.285448 \nL 47.285612 85.175594 \nL 47.510928 83.319944 \nL 47.523701 83.210089 \nL 47.633737 82.182802 \nL 47.634317 82.182802 \nL 47.646612 82.072947 \nL 47.757692 81.125823 \nL 47.775193 81.015969 \nL 47.884095 80.285585 \nL 47.886202 80.285585 \nL 47.909219 80.1787 \nL 48.020132 79.578954 \nL 48.039217 79.469099 \nL 48.149848 78.759499 \nL 48.149852 78.759499 \nL 48.165506 78.652613 \nL 48.27646 77.972703 \nL 48.294572 77.865818 \nL 48.40527 77.257165 \nL 48.417106 77.162155 \nL 48.52804 76.580224 \nL 48.546107 76.470369 \nL 48.654276 75.87953 \nL 48.657037 75.87953 \nL 48.684289 75.769676 \nL 48.792989 75.23228 \nL 48.794955 75.23228 \nL 48.819405 75.122425 \nL 48.928674 74.469236 \nL 48.930454 74.469236 \nL 48.955161 74.359382 \nL 49.06333 73.875428 \nL 49.065961 73.875428 \nL 49.094701 73.765574 \nL 49.205222 73.341001 \nL 49.242271 73.231147 \nL 49.35337 72.759069 \nL 49.383488 72.649215 \nL 49.494341 72.156354 \nL 49.52233 72.0465 \nL 49.63305 71.657556 \nL 49.665521 71.547701 \nL 49.773947 71.11719 \nL 49.774951 71.11719 \nL 49.807232 71.010305 \nL 49.916106 70.59167 \nL 49.917997 70.59167 \nL 49.951884 70.481816 \nL 50.062791 70.078027 \nL 50.062809 70.078027 \nL 50.096981 69.968172 \nL 50.207637 69.650485 \nL 50.207768 69.650485 \nL 50.252072 69.54063 \nL 50.363067 69.20216 \nL 50.392088 69.092305 \nL 50.502087 68.753835 \nL 50.502967 68.753835 \nL 50.542819 68.64398 \nL 50.653552 68.234253 \nL 50.653767 68.234253 \nL 50.69082 68.124398 \nL 50.793635 67.803742 \nL 50.801275 67.803742 \nL 50.827126 67.693888 \nL 50.937297 67.385108 \nL 50.938091 67.385108 \nL 50.965412 67.275253 \nL 51.075679 67.046637 \nL 51.076496 67.046637 \nL 51.123576 66.936783 \nL 51.234033 66.681445 \nL 51.234599 66.681445 \nL 51.279438 66.571591 \nL 51.390446 66.369696 \nL 51.390456 66.369696 \nL 51.447614 66.259841 \nL 51.556674 65.998566 \nL 51.557127 65.998566 \nL 51.604836 65.888711 \nL 51.713684 65.630405 \nL 51.715691 65.630405 \nL 51.756299 65.520551 \nL 51.866205 65.238492 \nL 51.916006 65.128637 \nL 52.027066 64.920805 \nL 52.073703 64.81095 \nL 52.182718 64.540767 \nL 52.184355 64.540767 \nL 52.21862 64.430913 \nL 52.325121 64.181514 \nL 52.329703 64.181514 \nL 52.393726 64.071659 \nL 52.499161 63.765848 \nL 52.504673 63.765848 \nL 52.54652 63.655994 \nL 52.655793 63.427378 \nL 52.657229 63.427378 \nL 52.713303 63.317523 \nL 52.821926 63.115628 \nL 52.823921 63.115628 \nL 52.889597 63.005774 \nL 52.99167 62.732622 \nL 53.000342 62.732622 \nL 53.083481 62.622768 \nL 53.186772 62.376337 \nL 53.193356 62.376337 \nL 53.246295 62.266483 \nL 53.356723 62.046774 \nL 53.357226 62.046774 \nL 53.419985 61.93692 \nL 53.529927 61.740963 \nL 53.530856 61.740963 \nL 53.609582 61.631109 \nL 53.715508 61.44109 \nL 53.720589 61.44109 \nL 53.785318 61.331235 \nL 53.895 61.144186 \nL 53.896248 61.144186 \nL 53.980341 61.034332 \nL 54.089667 60.862127 \nL 54.091264 60.862127 \nL 54.143445 60.752273 \nL 54.252247 60.556316 \nL 54.254072 60.556316 \nL 54.325898 60.446462 \nL 54.4368 60.256443 \nL 54.436935 60.256443 \nL 54.486135 60.149558 \nL 54.596535 59.968446 \nL 54.596536 59.968446 \nL 54.677149 59.858592 \nL 54.787746 59.689357 \nL 54.788092 59.689357 \nL 54.863219 59.579502 \nL 54.966043 59.425112 \nL 54.974327 59.425112 \nL 55.017386 59.318227 \nL 55.126953 59.125239 \nL 55.127582 59.125239 \nL 55.193228 59.015384 \nL 55.304183 58.807552 \nL 55.379271 58.697697 \nL 55.488969 58.543307 \nL 55.489426 58.543307 \nL 55.583583 58.433453 \nL 55.692915 58.243434 \nL 55.693969 58.243434 \nL 55.762764 58.13358 \nL 55.87186 57.955437 \nL 55.873858 57.955437 \nL 55.935634 57.845583 \nL 56.040603 57.694162 \nL 56.04674 57.694162 \nL 56.118367 57.584307 \nL 56.228085 57.370536 \nL 56.229342 57.370536 \nL 56.305604 57.260682 \nL 56.416037 57.079571 \nL 56.416678 57.079571 \nL 56.506438 56.969716 \nL 56.616544 56.806419 \nL 56.616916 56.806419 \nL 56.691068 56.696564 \nL 56.799949 56.530298 \nL 56.802063 56.530298 \nL 56.899992 56.420444 \nL 57.005164 56.283868 \nL 57.010321 56.283868 \nL 57.117943 56.176983 \nL 57.227125 55.992902 \nL 57.228886 55.992902 \nL 57.310275 55.883048 \nL 57.419001 55.743503 \nL 57.420945 55.743503 \nL 57.515768 55.633648 \nL 57.611527 55.425815 \nL 57.626621 55.425815 \nL 57.763319 55.315961 \nL 57.873337 55.200168 \nL 57.873526 55.200168 \nL 57.95969 55.090314 \nL 58.070659 54.974521 \nL 58.177884 54.864667 \nL 58.287312 54.739967 \nL 58.288936 54.739967 \nL 58.37747 54.630113 \nL 58.468603 54.523227 \nL 58.486844 54.523227 \nL 58.563091 54.413373 \nL 58.670585 54.29758 \nL 58.67356 54.29758 \nL 58.778623 54.187726 \nL 58.886268 54.077871 \nL 58.889478 54.077871 \nL 58.96549 53.968017 \nL 59.075743 53.80472 \nL 59.076532 53.80472 \nL 59.173397 53.694865 \nL 59.28166 53.567197 \nL 59.284484 53.567197 \nL 59.35704 53.457342 \nL 59.467145 53.294045 \nL 59.467604 53.294045 \nL 59.579278 53.18419 \nL 59.689395 53.098088 \nL 59.690363 53.098088 \nL 59.806231 52.988234 \nL 59.916844 52.863534 \nL 60.025165 52.75368 \nL 60.134823 52.62898 \nL 60.135375 52.62898 \nL 60.250271 52.519126 \nL 60.359549 52.403333 \nL 60.361122 52.403333 \nL 60.508182 52.293479 \nL 60.618443 52.198469 \nL 60.618447 52.198469 \nL 60.718666 52.088615 \nL 60.828756 51.972822 \nL 60.829652 51.972822 \nL 60.959045 51.862968 \nL 61.069342 51.785773 \nL 61.069794 51.785773 \nL 61.182899 51.675918 \nL 61.292684 51.569033 \nL 61.29324 51.569033 \nL 61.433871 51.459178 \nL 61.538451 51.29885 \nL 61.544733 51.29885 \nL 61.658727 51.188996 \nL 61.768885 51.088048 \nL 61.76971 51.088048 \nL 61.932347 50.978194 \nL 62.041199 50.862401 \nL 62.043436 50.862401 \nL 62.155717 50.752547 \nL 62.260904 50.68129 \nL 62.265969 50.68129 \nL 62.391397 50.571436 \nL 62.496052 50.479395 \nL 62.502425 50.479395 \nL 62.633973 50.369541 \nL 62.737926 50.256717 \nL 62.744984 50.256717 \nL 62.853337 50.146863 \nL 62.963599 50.028101 \nL 63.073366 49.918247 \nL 63.183754 49.835114 \nL 63.336863 49.725259 \nL 63.444589 49.615405 \nL 63.447603 49.615405 \nL 63.592295 49.50555 \nL 63.697992 49.401634 \nL 63.703115 49.401634 \nL 63.843639 49.291779 \nL 63.9547 49.214584 \nL 64.098479 49.10473 \nL 64.207848 49.021597 \nL 64.209276 49.021597 \nL 64.405969 48.914711 \nL 64.514362 48.82564 \nL 64.516979 48.82564 \nL 64.632643 48.715786 \nL 64.741048 48.629684 \nL 64.742856 48.629684 \nL 64.929338 48.519829 \nL 65.040396 48.409975 \nL 65.204971 48.30012 \nL 65.314836 48.20808 \nL 65.316022 48.20808 \nL 65.475607 48.098225 \nL 65.584297 48.009154 \nL 65.586106 48.009154 \nL 65.732064 47.902269 \nL 65.840547 47.80726 \nL 65.842845 47.80726 \nL 66.016932 47.697405 \nL 66.124192 47.62021 \nL 66.127783 47.62021 \nL 66.302349 47.510356 \nL 66.409167 47.406439 \nL 66.413381 47.406439 \nL 66.5435 47.296585 \nL 66.640946 47.21939 \nL 66.654162 47.21939 \nL 66.837657 47.109535 \nL 66.945449 47.053123 \nL 66.948714 47.053123 \nL 67.107693 46.943269 \nL 67.21844 46.889826 \nL 67.420971 46.779972 \nL 67.5249 46.696839 \nL 67.531141 46.696839 \nL 67.722719 46.586984 \nL 67.832773 46.512758 \nL 67.832971 46.512758 \nL 68.025393 46.402904 \nL 68.124052 46.319771 \nL 68.136098 46.319771 \nL 68.357746 46.209916 \nL 68.468513 46.144597 \nL 68.643091 46.034743 \nL 68.75304 45.945672 \nL 68.753369 45.945672 \nL 68.920637 45.835817 \nL 69.030983 45.717056 \nL 69.031514 45.717056 \nL 69.229402 45.607201 \nL 69.330712 45.550789 \nL 69.340151 45.550789 \nL 69.515821 45.440935 \nL 69.618123 45.351864 \nL 69.626866 45.351864 \nL 69.773628 45.242009 \nL 69.87763 45.164814 \nL 69.884598 45.164814 \nL 70.052765 45.05496 \nL 70.161324 44.989641 \nL 70.163333 44.989641 \nL 70.284055 44.879786 \nL 70.393093 44.808529 \nL 70.393973 44.808529 \nL 70.590417 44.698675 \nL 70.701105 44.62148 \nL 70.701236 44.62148 \nL 70.875727 44.511625 \nL 70.980412 44.416616 \nL 70.986511 44.416616 \nL 71.208477 44.306762 \nL 71.320779 44.271133 \nL 71.538433 44.161279 \nL 71.648616 44.054393 \nL 71.846309 43.944539 \nL 71.953847 43.867344 \nL 71.957281 43.867344 \nL 72.170104 43.757489 \nL 72.277508 43.674356 \nL 72.280395 43.674356 \nL 72.470913 43.564502 \nL 72.581272 43.50809 \nL 72.874432 43.398236 \nL 72.978147 43.344793 \nL 72.985401 43.344793 \nL 73.147075 43.234938 \nL 73.25816 43.184465 \nL 73.449028 43.07461 \nL 73.54515 42.994446 \nL 73.559171 42.994446 \nL 73.861717 42.884592 \nL 73.957544 42.819273 \nL 73.972677 42.819273 \nL 74.163731 42.709418 \nL 74.261934 42.650037 \nL 74.27467 42.650037 \nL 74.485582 42.540183 \nL 74.591297 42.477833 \nL 74.59582 42.477833 \nL 74.779424 42.367979 \nL 74.880773 42.290784 \nL 74.890453 42.290784 \nL 75.087913 42.180929 \nL 75.195269 42.130456 \nL 75.198701 42.130456 \nL 75.486158 42.020601 \nL 75.595424 41.955282 \nL 75.596178 41.955282 \nL 75.784529 41.845428 \nL 75.892103 41.809799 \nL 75.894075 41.809799 \nL 76.119159 41.699945 \nL 76.227114 41.640564 \nL 76.229548 41.640564 \nL 76.452249 41.53071 \nL 76.555154 41.486174 \nL 76.563119 41.486174 \nL 76.759935 41.376319 \nL 76.865029 41.302093 \nL 76.870918 41.302093 \nL 77.131223 41.192239 \nL 77.237193 41.123951 \nL 77.242028 41.123951 \nL 77.596535 41.014097 \nL 77.703496 40.954716 \nL 77.705484 40.954716 \nL 77.98801 40.844861 \nL 78.097524 40.779542 \nL 78.099034 40.779542 \nL 78.313493 40.669688 \nL 78.410679 40.622183 \nL 78.422316 40.622183 \nL 78.678 40.512329 \nL 78.788499 40.452948 \nL 78.788687 40.452948 \nL 78.99748 40.343094 \nL 79.098948 40.26293 \nL 79.108462 40.26293 \nL 79.380631 40.153075 \nL 79.490189 40.117447 \nL 79.49076 40.117447 \nL 79.749075 40.007592 \nL 79.85758 39.957118 \nL 79.859734 39.957118 \nL 80.096327 39.847264 \nL 80.188475 39.799759 \nL 80.207409 39.799759 \nL 80.494137 39.689905 \nL 80.600948 39.624586 \nL 80.604853 39.624586 \nL 80.869639 39.514732 \nL 80.978286 39.464258 \nL 80.980419 39.464258 \nL 81.248057 39.354403 \nL 81.356683 39.309868 \nL 81.359012 39.309868 \nL 81.674036 39.200013 \nL 81.773385 39.155478 \nL 81.785006 39.155478 \nL 82.160748 39.045623 \nL 82.270134 38.998119 \nL 82.271261 38.998119 \nL 82.584095 38.888264 \nL 82.692454 38.855605 \nL 82.693486 38.855605 \nL 83.022707 38.74575 \nL 83.118913 38.71606 \nL 83.133768 38.71606 \nL 83.437044 38.606205 \nL 83.521412 38.558701 \nL 83.547737 38.558701 \nL 83.73497 38.448846 \nL 83.834545 38.392435 \nL 83.845888 38.392435 \nL 84.196773 38.28258 \nL 84.307492 38.223199 \nL 84.307496 38.223199 \nL 84.528341 38.113345 \nL 84.622755 38.068809 \nL 84.638226 38.068809 \nL 85.021855 37.958955 \nL 85.129053 37.905512 \nL 85.132624 37.905512 \nL 85.427299 37.795658 \nL 85.536337 37.765967 \nL 85.538262 37.765967 \nL 85.805792 37.656113 \nL 85.886695 37.608608 \nL 85.916059 37.608608 \nL 86.287986 37.498754 \nL 86.39726 37.44828 \nL 86.399078 37.44828 \nL 86.724292 37.338425 \nL 86.831893 37.296859 \nL 86.833707 37.296859 \nL 87.257469 37.187004 \nL 87.360779 37.133562 \nL 87.368179 37.133562 \nL 87.647892 37.023707 \nL 87.743162 36.98511 \nL 87.758684 36.98511 \nL 88.057616 36.875255 \nL 88.168464 36.818843 \nL 88.505742 36.708989 \nL 88.616724 36.664453 \nL 88.833635 36.554599 \nL 88.937254 36.510063 \nL 88.944159 36.510063 \nL 89.188625 36.400209 \nL 89.285055 36.367549 \nL 89.299358 36.367549 \nL 89.562499 36.257695 \nL 89.672585 36.207221 \nL 89.673411 36.207221 \nL 90.020209 36.097367 \nL 90.127758 36.064707 \nL 90.130637 36.064707 \nL 90.420498 35.954853 \nL 90.515139 35.910317 \nL 90.531532 35.910317 \nL 90.861179 35.800463 \nL 90.971106 35.764834 \nL 90.971874 35.764834 \nL 91.32168 35.65498 \nL 91.420309 35.631228 \nL 91.431818 35.631228 \nL 91.696705 35.521373 \nL 91.802014 35.482776 \nL 91.807462 35.482776 \nL 92.246919 35.372921 \nL 92.350747 35.334324 \nL 92.358015 35.334324 \nL 92.746175 35.224469 \nL 92.810656 35.203686 \nL 92.856486 35.203686 \nL 93.185007 35.093831 \nL 93.296177 35.055234 \nL 93.616491 34.945379 \nL 93.701719 34.91272 \nL 93.727221 34.91272 \nL 94.106457 34.805835 \nL 94.205115 34.782082 \nL 94.217205 34.782082 \nL 94.628854 34.675197 \nL 94.739646 34.627692 \nL 94.739907 34.627692 \nL 95.12682 34.517838 \nL 95.230295 34.488147 \nL 95.237446 34.488147 \nL 95.766536 34.378293 \nL 95.865954 34.345633 \nL 95.877458 34.345633 \nL 96.318554 34.235779 \nL 96.427698 34.185305 \nL 96.428588 34.185305 \nL 96.926126 34.075451 \nL 97.027609 34.030915 \nL 97.037068 34.030915 \nL 97.377789 33.921061 \nL 97.480087 33.885432 \nL 97.488887 33.885432 \nL 97.997633 33.775578 \nL 98.107402 33.745887 \nL 98.108287 33.745887 \nL 98.506553 33.636033 \nL 98.609714 33.606342 \nL 98.617605 33.606342 \nL 99.051951 33.496488 \nL 99.1591 33.451952 \nL 99.162558 33.451952 \nL 99.628294 33.342098 \nL 99.733821 33.315377 \nL 99.739361 33.315377 \nL 100.251055 33.205522 \nL 100.349056 33.166925 \nL 100.361643 33.166925 \nL 100.671084 33.05707 \nL 100.727611 33.036287 \nL 100.782125 33.036287 \nL 101.208824 32.926432 \nL 101.31583 32.896742 \nL 101.319158 32.896742 \nL 101.671402 32.786888 \nL 101.771913 32.733445 \nL 101.782269 32.733445 \nL 102.178332 32.62359 \nL 102.286805 32.5939 \nL 102.288816 32.5939 \nL 102.738358 32.484045 \nL 102.844965 32.430603 \nL 102.849361 32.430603 \nL 103.369552 32.320748 \nL 103.456839 32.288089 \nL 103.480524 32.288089 \nL 103.948143 32.178234 \nL 104.049276 32.133699 \nL 104.058328 32.133699 \nL 104.621148 32.023844 \nL 104.723349 31.979309 \nL 104.73208 31.979309 \nL 105.329826 31.869454 \nL 105.42896 31.833826 \nL 105.44084 31.833826 \nL 105.991847 31.723971 \nL 106.062732 31.703188 \nL 106.10295 31.703188 \nL 106.53138 31.593334 \nL 106.612631 31.563643 \nL 106.642005 31.563643 \nL 107.150454 31.453789 \nL 107.257275 31.415191 \nL 107.261524 31.415191 \nL 107.8077 31.305337 \nL 107.899821 31.281584 \nL 107.918792 31.281584 \nL 108.457733 31.17173 \nL 108.554538 31.13907 \nL 108.567885 31.13907 \nL 109.222929 31.029216 \nL 109.305894 30.993588 \nL 109.333052 30.993588 \nL 109.81611 30.883733 \nL 109.923915 30.854043 \nL 109.927078 30.854043 \nL 110.374771 30.744188 \nL 110.470406 30.714498 \nL 110.485631 30.714498 \nL 110.909666 30.604643 \nL 111.019895 30.574953 \nL 111.020766 30.574953 \nL 111.61544 30.465098 \nL 111.724333 30.435408 \nL 111.726471 30.435408 \nL 112.314837 30.325554 \nL 112.424875 30.286956 \nL 112.959738 30.177102 \nL 113.051563 30.159287 \nL 113.069839 30.159287 \nL 113.576002 30.049433 \nL 113.658494 30.016773 \nL 113.68684 30.016773 \nL 114.225659 29.906919 \nL 114.338446 29.865352 \nL 115.158609 29.755498 \nL 115.228948 29.731746 \nL 115.269432 29.731746 \nL 115.796451 29.621891 \nL 115.89764 29.601108 \nL 115.907375 29.601108 \nL 116.49183 29.491253 \nL 116.561141 29.476408 \nL 116.602096 29.476408 \nL 117.338925 29.366554 \nL 117.413067 29.348739 \nL 117.449875 29.348739 \nL 117.918075 29.238885 \nL 118.023666 29.212164 \nL 118.029074 29.212164 \nL 118.753312 29.102309 \nL 118.862925 29.06965 \nL 118.864064 29.06965 \nL 119.350192 28.959795 \nL 119.456933 28.933074 \nL 119.460468 28.933074 \nL 120.018364 28.823219 \nL 120.125852 28.808374 \nL 120.129382 28.808374 \nL 120.679287 28.69852 \nL 120.752972 28.680706 \nL 120.789746 28.680706 \nL 121.336266 28.570851 \nL 121.445685 28.556006 \nL 121.44731 28.556006 \nL 122.081722 28.446151 \nL 122.188862 28.416461 \nL 122.192815 28.416461 \nL 122.705242 28.306607 \nL 122.806458 28.276916 \nL 122.81635 28.276916 \nL 123.460337 28.167062 \nL 123.566215 28.146278 \nL 123.571376 28.146278 \nL 124.445792 28.036424 \nL 124.553001 28.015641 \nL 124.556836 28.015641 \nL 125.474815 27.905786 \nL 125.575325 27.885003 \nL 125.584807 27.885003 \nL 126.439375 27.775148 \nL 126.489297 27.763272 \nL 126.550136 27.763272 \nL 127.344026 27.653418 \nL 127.449331 27.623727 \nL 127.45507 27.623727 \nL 128.086431 27.513873 \nL 128.198436 27.49309 \nL 129.009923 27.383235 \nL 129.118536 27.353545 \nL 129.120799 27.353545 \nL 129.655215 27.24369 \nL 129.763263 27.219938 \nL 129.766248 27.219938 \nL 130.457305 27.110084 \nL 130.545975 27.092269 \nL 130.567988 27.092269 \nL 131.182929 26.982415 \nL 131.280146 26.96757 \nL 131.292288 26.96757 \nL 131.967815 26.857715 \nL 132.068389 26.836932 \nL 132.078667 26.836932 \nL 132.886334 26.727077 \nL 132.993792 26.715201 \nL 132.997365 26.715201 \nL 133.603907 26.605347 \nL 133.654902 26.59644 \nL 133.714985 26.59644 \nL 134.348293 26.486585 \nL 134.437422 26.459864 \nL 134.458512 26.459864 \nL 135.227141 26.350009 \nL 135.257633 26.341102 \nL 135.337255 26.341102 \nL 136.11565 26.231248 \nL 136.219857 26.201557 \nL 136.226655 26.201557 \nL 137.032672 26.091703 \nL 137.135669 26.07092 \nL 137.143718 26.07092 \nL 137.912121 25.961065 \nL 138.007262 25.952158 \nL 138.023148 25.952158 \nL 138.856551 25.842304 \nL 138.932732 25.824489 \nL 138.967572 25.824489 \nL 139.896581 25.714635 \nL 140.007466 25.696821 \nL 140.007605 25.696821 \nL 140.987877 25.586966 \nL 141.098383 25.560245 \nL 141.09897 25.560245 \nL 142.034251 25.45039 \nL 142.141183 25.444452 \nL 142.145193 25.444452 \nL 143.057578 25.337567 \nL 143.149231 25.319753 \nL 143.168658 25.319753 \nL 144.125633 25.209898 \nL 144.221913 25.195053 \nL 144.236333 25.195053 \nL 145.0035 25.085198 \nL 145.105422 25.064415 \nL 145.114466 25.064415 \nL 145.763089 24.954561 \nL 145.818277 24.945654 \nL 145.87412 24.945654 \nL 147.295489 24.835799 \nL 147.404882 24.812047 \nL 147.40632 24.812047 \nL 148.459403 24.702192 \nL 148.514686 24.681409 \nL 148.57014 24.681409 \nL 149.511747 24.571555 \nL 149.601703 24.550771 \nL 149.622818 24.550771 \nL 150.562658 24.440917 \nL 150.624795 24.426072 \nL 150.673323 24.426072 \nL 151.299458 24.316217 \nL 151.406733 24.292465 \nL 151.410544 24.292465 \nL 152.249553 24.18261 \nL 152.349339 24.167765 \nL 152.36065 24.167765 \nL 153.254936 24.057911 \nL 153.351513 24.037127 \nL 153.365631 24.037127 \nL 154.607941 23.927273 \nL 154.653599 23.915397 \nL 154.718864 23.915397 \nL 155.866192 23.805542 \nL 155.970714 23.793666 \nL 155.977149 23.793666 \nL 157.275352 23.683812 \nL 157.384439 23.665997 \nL 157.386231 23.665997 \nL 158.193595 23.556143 \nL 158.258272 23.538329 \nL 158.30464 23.538329 \nL 159.428225 23.428474 \nL 159.517741 23.416598 \nL 159.53904 23.416598 \nL 160.564433 23.306744 \nL 160.663068 23.288929 \nL 160.675398 23.288929 \nL 162.064369 23.179075 \nL 162.177064 23.155323 \nL 163.418964 23.045468 \nL 163.526039 23.036561 \nL 163.530062 23.036561 \nL 165.006296 22.926707 \nL 165.104915 22.91483 \nL 165.117297 22.91483 \nL 166.464524 22.804976 \nL 166.574796 22.790131 \nL 166.575591 22.790131 \nL 167.517838 22.680276 \nL 167.554196 22.674338 \nL 167.628347 22.674338 \nL 169.298857 22.564484 \nL 169.399968 22.549639 \nL 169.409258 22.549639 \nL 170.807607 22.439784 \nL 170.909054 22.419001 \nL 170.918303 22.419001 \nL 172.199223 22.309146 \nL 172.284733 22.303208 \nL 172.310079 22.303208 \nL 173.836958 22.193354 \nL 173.930202 22.178509 \nL 173.946711 22.178509 \nL 175.395945 22.068654 \nL 175.429073 22.062716 \nL 175.506921 22.062716 \nL 177.216619 21.952862 \nL 177.295013 21.940985 \nL 177.327575 21.940985 \nL 178.717523 21.831131 \nL 178.825442 21.822224 \nL 178.8286 21.822224 \nL 180.62995 21.712369 \nL 180.648914 21.706431 \nL 180.740424 21.706431 \nL 182.568077 21.596577 \nL 182.665063 21.584701 \nL 182.679051 21.584701 \nL 184.298343 21.474846 \nL 184.372377 21.468908 \nL 184.408874 21.468908 \nL 185.975203 21.359054 \nL 185.975203 21.356085 \nL 186.085903 21.356085 \nL 187.72979 21.24623 \nL 187.797573 21.234354 \nL 187.840876 21.234354 \nL 189.264619 21.124499 \nL 189.314154 21.115592 \nL 189.373947 21.115592 \nL 191.422561 21.005738 \nL 191.478543 20.9998 \nL 191.532493 20.9998 \nL 193.60856 20.889945 \nL 193.702363 20.881038 \nL 193.71958 20.881038 \nL 195.244886 20.771184 \nL 195.296999 20.762277 \nL 195.355915 20.762277 \nL 196.861511 20.652422 \nL 196.974318 20.649453 \nL 200.546531 20.539599 \nL 200.643411 20.530691 \nL 200.657462 20.530691 \nL 201.898952 20.420837 \nL 201.898952 20.417868 \nL 202.009956 20.417868 \nL 204.228475 20.308014 \nL 204.228475 20.305044 \nL 204.339502 20.305044 \nL 205.661901 20.19519 \nL 205.741128 20.189252 \nL 205.772306 20.189252 \nL 208.41901 20.079397 \nL 208.529198 20.073459 \nL 208.529848 20.073459 \nL 210.291877 19.963605 \nL 210.379372 19.94876 \nL 210.402765 19.94876 \nL 212.896231 19.838905 \nL 212.962605 19.832967 \nL 213.006837 19.832967 \nL 215.16963 19.723113 \nL 215.224859 19.708267 \nL 215.279385 19.708267 \nL 217.5178 19.598413 \nL 217.611822 19.586537 \nL 217.628851 19.586537 \nL 221.031562 19.476682 \nL 221.042649 19.470744 \nL 221.142664 19.470744 \nL 223.703745 19.36089 \nL 223.792693 19.354952 \nL 223.814305 19.354952 \nL 226.539411 19.245097 \nL 226.63992 19.233221 \nL 226.650121 19.233221 \nL 229.16596 19.123367 \nL 229.25237 19.111491 \nL 229.27705 19.111491 \nL 231.994164 19.001636 \nL 232.046683 18.992729 \nL 232.103965 18.992729 \nL 235.252779 18.882874 \nL 235.325765 18.868029 \nL 235.362083 18.868029 \nL 239.545523 18.758175 \nL 239.641646 18.746299 \nL 239.656192 18.746299 \nL 243.905069 18.636444 \nL 243.905877 18.630506 \nL 244.015759 18.630506 \nL 248.040562 18.520652 \nL 248.088497 18.514714 \nL 248.150666 18.514714 \nL 251.629413 18.404859 \nL 251.629413 18.40189 \nL 251.739385 18.40189 \nL 256.408885 18.292036 \nL 256.476035 18.286097 \nL 256.519735 18.286097 \nL 261.984894 18.176243 \nL 261.984894 18.173274 \nL 262.09599 18.173274 \nL 267.257762 18.063419 \nL 267.303954 18.057481 \nL 267.367787 18.057481 \nL 272.706357 17.947627 \nL 272.716307 17.941689 \nL 272.817091 17.941689 \nL 277.606193 17.831834 \nL 277.659677 17.825896 \nL 277.71453 17.825896 \nL 283.082756 17.716042 \nL 283.082756 17.713073 \nL 283.193342 17.713073 \nL 289.713548 17.603218 \nL 289.760912 17.59728 \nL 289.824392 17.59728 \nL 297.968089 17.487426 \nL 297.968089 17.484457 \nL 298.078943 17.484457 \nL 304.365175 17.374602 \nL 304.365175 17.371633 \nL 304.476266 17.371633 \nL 315.419796 17.261779 \nL 315.419796 17.25881 \nL 315.53066 17.25881 \nL 334.967006 17.148955 \nL 349.684943 17.083636 \nL 349.684943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p14001b091d)\" d=\"M 45.321307 214.756364 \nL 349.684943 17.083636 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p14001b091d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fpr = False positive rate\n",
    "# tpr = True positive rate\n",
    "fpr, tpr, thr = roc_curve(test_label , test_predictions, sample_weight=test_data[\"Weights\"])\n",
    "plt.plot(fpr, tpr, label='ROC')\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9551403101654902"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(test_label , test_predictions, sample_weight=test_data[\"Weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}